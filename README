# FeelIt - Opinion Analysis on Reddit

**FeelIt** is a web application developed as a Final Degree Project to analyze opinions on any topic collected from Reddit, presenting a summarized and clear view through sentiment analysis and highlighted comments.

---

## Reddit Opinion Extractor — Backend

This is the backend service for a Reddit opinion analysis app.
It uses FastAPI, PRAW (Reddit API), and a fine-tuned DistilBERT model to extract, filter and classify Reddit comments based on a user-provided keyword.

---

## Project Structure

/Backend/
│
├── app/
│   ├── main.py                    ← FastAPI entrypoint
│   ├── services/                  ← Core logic
│   │   ├── findPosts.py         ← Reddit search and CSV export
│   │   ├── analyzeSentiment.py   ← Sentiment analysis
│   │   └── sortTitles.py        ← Filter titles using DistilBERT
│   │
│   └── models/
│       └── model_output/             ← Trained DistilBERT model
│
├── data/
│   ├── CSVfile/                  ← Raw Reddit results
│   ├── CSVfilefiltered/         ← Filtered Reddit results
│   ├── CSVsTraining/            ← Data used for model training
│   ├── ResultadosFiltered/      ← Cleaned, classified results
│   ├── ResultadosLabeled/       ← Labeled datasets
│   └── titles_dataset_expanded_pipe.txt
│
├── scripts/
│   ├── EntrenarModelo.py        ← Train DistilBERT model
│   ├── ProvarModelo.py          ← Test the trained model
│   ├── printUniqueTitles.py     ← View unique titles from dataset
│   └── sortTitles_training.py   ← Filter logic used for training
│
├── .env                         ← Contains Reddit API credentials
├── config.init                  ← Optional configuration script
├── requirements.txt             ← Python dependencies
└── README.md

---

## Installation

This project requires a pre-trained model (~1.74 GB) to work properly.  
The model is **not included in this repository** due to its size.

**Download the model here**: [https://drive.google.com/drive/folders/11-DXYhMhrijQPiZU600u68px3JMKUmTb?usp=drive_link]

After downloading, place the model inside the project folder as follows, it should look like this:
```
Backend/
├── app/
│    └── models/
│        └── model_output/             ← Inside this folder, place the downloaded model
```

---

 ## How It Works

1. A user sends a search keyword to the FastAPI backend.

2. The /api/search route triggers:
    · findPosts.py → searches Reddit and stores comments in a CSV file.
    · sortTitles.py → uses a trained DistilBERT model to filter relevant posts.

3. The filtered CSV can then be used for further sentiment analysis or visualization.

---

## How to Run (Local Development)

**1. Clone the project and create a virtual environment**
"python -m venv venv"

**2. Activate the virtual environment**
# Windows
".\venv\Scripts\activate"

**3. Install dependencies**
"pip install -r requirements.txt"

**4. Set up .env file with Reddit API credentials**
REDDIT_CLIENT_ID=your_id
REDDIT_CLIENT_SECRET=your_secret
REDDIT_USER_AGENT=your_user_agent

OPENAI_API_KEY=your_openai_api_key
OPENAI_MODEL=the_model_you_want_to_use # e.g., gpt-3.5-turbo

**5. Run the FastAPI server**
"uvicorn app.main:app --reload --port 8000"


**Backend will now be accessible at:**
`http://localhost:8000/api/search?keyword=example`

---

## Model Training & Testing

1. Train the DistilBERT model using the training data (sortTitles_training.py).
2. Test the model using the test data (ProvarModelo.py).

===================

## Data Visualization

To train and test the classifier:

1. Extract titles from labeled data:
"python scripts/printUniqueTitles.py"

2. Train the model:
"python scripts/EntrenarModelo.py"

3. Evaluate or test it:
"python scripts/ProvarModelo.py"

---

## Requirements

All required packages are listed in requirements.txt.

---

## Developed by

**Pol González Casals**  
Universitat Autònoma de Barcelona (2025)  
[LinkedIn](https://www.linkedin.com/in/pol-gonzalez-casals) · [GitHub](https://github.com/ppolsii)

